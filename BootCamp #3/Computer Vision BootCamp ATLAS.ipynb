{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"\"\n",
        "author: \"\"\n",
        "format: html\n",
        "editor: visual\n",
        "---"
      ],
      "id": "680c3bc9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BootCamp ATLAS #3\n",
        "\n",
        "***Author : Mateusz Walo***\n",
        "\n",
        "# Wstęp do uczenia głębokiego\n",
        "\n",
        "Czym jest uczenie głębokie i jak ma się w stosunku do klasycznego uczenia maszynowego które poznałeś na poprzednich zajęciach?\n",
        "\n",
        "![](mldl.png)\n",
        "\n",
        "## Sieci neuronowe - idea\n",
        "\n",
        "Czym są sieci neuronowe? Z czego się składają? Dlaczego są tak istotne w wizji komputerowej?\n",
        "\n",
        "![](nn_image_bootcamp.png){fig-alt=\"Model sieci neuronowej\"}\n",
        "\n",
        "## Perceptron\n",
        "\n",
        "![](Perceptron.png)\n",
        "\n",
        "Prosty algorytm który dla wektora `x` o `n` wartościach, `x=(x1,x2,x3,...xn)` nazywanych w przyszłości cechami *(ang. features)* zwraca wynik 0 (fałsz) albo 1 (prawda)\n",
        "\n",
        "$$\n",
        "f(x) = \n",
        "\\begin{cases} \n",
        "1 & \\text{jeśli } wx + b > 0 \\\\\n",
        "0 & \\text{w każdym innym przypadku}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\\\n",
        "Gdzie `b` symbolizuje ***bias***, `w` jest wektorem wag a `wx` jest wynikiem sumy\n",
        "\n",
        "$$\n",
        "wx=\\sum_{j=1}^{m} w_j x_j\n",
        "$$\\\n",
        "\n",
        "***Bias** - Dodatkowy parametr dodawany do sumy ważonej w każdym neuronie, przesuwa on `funkcję aktywacji` w taki sposób aby sieć mogła dopasować się do danych*\n",
        "\n",
        "**Spostrzeżenie:** Jak pamiętacie z Geometrii Analitycznej `wx+b` stanowi hiperpłaszczyzne graniczną która zmienia swoje położenie w przestrzeni (w tym wypadku n-wymiarowej) w zależności od `w` i `b` 😉\n",
        "\n",
        "## Dlaczego Perceptron jest problematyczny w treningu?\n",
        "\n",
        "Weżmy pod uwagę pojedyńczy neuron, jak wybrać najlepsze wagi i bias?\n",
        "\n",
        "W ogólbym przypadku chcemy dostarczyć zbiór treningowy aby model sam ustalił wagę i bias, tak aby minimalizować liczbę błędów wraz z kolejnymi epokami sieci. Wyobraźmy sobie że mamy zbiór zdjęć żyraf (takie duże długoszyjne zwierze afrykańskie) i zbiór który nie zawiera ani jednej żyrafy, dokonajmy założenia że każdy neuron wejściowy przyjmuje dane z jednego piksela na zdjęciu. Podczas gdy komputer przetwarza te obrazy, chcielibyśmy, aby nasz neuron dostosowywał swoje wagi i bias, abyśmy mieli coraz mniej obrazów błędnie rozpoznawanych jako stworzenia będące nie-żyrafami. Wszystko byłoby pięknie gdyby niewielka zmiana wartości `w` i `b` powodowała tylko niewielką zmianę w predykcji sieci. Natomiast mamy duży skok wyjściowy, co uniemożliwia nam progresywne uczenie sieci. Co prezentuje wykres poniżej\n",
        "\n",
        "![](pppp.png){fig-align=\"center\" width=\"500\"}\n",
        "\n",
        "Matematycy obecni na sali z łatwością zauważą że dzieję się tak ponieważ ta funkcja nie jest ciągła w zerze 😉\n",
        "\n",
        "## Ratunek - funkcje aktywacyjne\n",
        "\n",
        "### \n",
        "\n",
        "***Sigmoid*** - tak nazywamy funkcję aktywacyjną daną wzorem\n",
        "\n",
        "$$\n",
        "\\sigma(x)=\\dfrac{1}{1+e^{-x}}\n",
        "$$\n",
        "\n",
        "Jej wykres prezentuje się następująco\n",
        "\n",
        "![Jak widzimy w tym wypadku funkcja jest ciągła w zerze, co pozwala sieci na odpowiedź \"może\" gdy nie jest pewna swojej predykcji w trudniejszych przypadkach. Jak dobrze pamiętacie z algebry liniowej istnieje coś takiego jak kombinacja liniowa, bez funkcji aktywacji wagi i bias przekazywane do nastepnych warstw bylyby tyko kombinacją liniową, nie zmieniałyby swoich wartości i siec nie mogłaby się uczyć.](Logistic-curve.png){fig-align=\"center\"}\n",
        "\n",
        "**ReLU -**tak nazywamy funkcję daną wzorem\n",
        "\n",
        "$$\n",
        "f(x)=max(0,x)\n",
        "$$\n",
        "\n",
        "której wykres prezentuje się następująco\n",
        "\n",
        "![](relu.png){fig-align=\"center\"}\n",
        "\n",
        "***Dla dociekliwych:** Istnieje kilka innych funkcji aktywacyjnych, które omówimy szczegółowo w dalszej części BootCampu 😉*\n",
        "\n",
        "## Funkcje straty\n",
        "\n",
        "**MSE** - To jest średni błąd kwadratowy między przewidywaniami a prawdziwymi wartościami. Matematycznie, jeśli `Y'` jest wektorem `n` predykcji, a `Y` jest wektorem `n` obserwowanych wartości, to spełniają one następujące równanie:\n",
        "\n",
        "$$\n",
        "MSE=\\dfrac{1}{n}\\sum_{i=1}^{n}(Y'-Y)^2\n",
        "$$\n",
        "\n",
        "**Spostrzeżenie:** *Dzięki tej funkcji możemy dostrzec że jeśli predykcje będą się różnić znacząco od wartości prawdziwych, to roznica bedzie widocza drastycznie, zobaczymy ten błąd do kwadratu!*\n",
        "\n",
        "**Binary cross-entropy** - Jest to binarna logarytmiczna strata. Załóżmy, że predykcja modelu to `p`, podczas gdy naszym targetem jest `t` wówczas binarna entropia krzyżowa jest zdefiniowana następująco:\n",
        "\n",
        "$$\n",
        "-tln(p)-(1-t)ln(1-p)\n",
        "$$\n",
        "\n",
        "**Spostrzeżenie:** Wykorzystywana najczęściej do klasyfikacji binarnej (na studiach będziecie jej używać najczęściej 😉)\n",
        "\n",
        "**Categorical cross-entropy -** oznaczenia podobnie jak wyżej\n",
        "\n",
        "$$\n",
        "L_i=-\\sum jt_{i,j}ln(p_{i,j})\n",
        "$$\n",
        "\n",
        "**Spostrzeżenie:** Wykorzystywana najczęściej do klasyfikacji wieloetykietowej (zaraz zobaczycie ją w akcji)\n",
        "\n",
        "## Optymalizator\n",
        "\n",
        "Wyobraźmy sobie ogólną funkcję kosztu C(w) jednej zmiennej jak na poniższym wykresie:\n",
        "\n",
        "![](opt.png){fig-align=\"center\"}\n",
        "\n",
        "Schodzenie po gradiencie można postrzegać jako wędrowca, który zamierza zejść z góry do doliny. Góra reprezentuje funkcję `C`, podczas gdy dolina reprezentuje minimalne `min` . Wędrowiec ma punkt początkowy `w0` . Wędrowiec porusza się powoli. Na każdym kroku `r` gradient jest kierunkiem maksymalnego wzrostu. Matematycznie ten kierunek jest wartością pochodnej cząstkowej w punkcie `wr` osiągniętym w kroku `r`. Dlatego też, wybierając przeciwny kierunek, wędrowiec może poruszać się w kierunku doliny. Na każdym kroku wędrowiec może decydować, jaką odległośc chce pokonać następnym krokiem. Jest to tempo uczenia się. Należy zauważyć, że jeśli jest zbyt małe, wędrowiec będzie poruszał się powoli. Jednak jeśli jest zbyt duże, wędrowiec prawdopodobnie ominie dolinę.\n",
        "\n",
        "## Realny przykład - Baza MNIST\n",
        "\n",
        "Baza danych MNIST zawiera 70 000 zdjęc odręcznie pisanych cyfr, w skali szarości, każde wymiaru 28x28 pikseli.\n",
        "\n",
        "Jak zaimportować tensorflow i MNIST?\n"
      ],
      "id": "d71616b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -U numpy --quiet"
      ],
      "id": "48a16650",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!pip install -U tensorflow pandas matplotlib --quiet\n",
        "#Instrukcja powyżej jeśli nie masz zainstalowanych bibliotek\n",
        "\n",
        "#Importujemy potrzebne biblioteki\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "d19573a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Ustalamy wartość podstawowych parametrów\n",
        "\n",
        "nb_epochs=10 #Liczba epok przez które będzie się trenować sieć\n",
        "val_split=0.2 #Czesc jaka zostanie przydzielona na zbior walidacyjny\n",
        "nb_class=10 #Liczba klas outputów, cyfry of 0 do 9\n",
        "nb_hidden=128 #Liczba warstw ukrytych\n",
        "reshaped=784 #do skalowania zdjęć, 784=28*28 co odpowiada wymiarowi zdjęc\n",
        "batch_size=128 #Liczba próbek która będzie przetwarzana w 1 obrocie\n",
        "optimizer=SGD() #Optymalizator Stochastic gradient descent"
      ],
      "id": "5539570d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Wczytujemy dane mnist\n",
        "\n",
        "(X_train, y_train),(X_test, y_test)=mnist.load_data()"
      ],
      "id": "f7e186f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Sprawdzmy czy dane wczytały się poprawnie\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(X_train[1])\n",
        "plt.title(f\"Na rysunku znajduję się cyfra {y_train[1]}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "5af930f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Przekształcamy, normalizujemy dane mnist\n",
        "\n",
        "X_train=X_train.reshape(60000, reshaped)\n",
        "X_test=X_test.reshape(10000, reshaped)\n",
        "X_train=X_train.astype(\"float32\")/255\n",
        "X_test=X_test.astype(\"float32\")/255\n",
        "y_train=to_categorical(y_train, nb_class) #Pamiętacie co to OneHotEncoding ;)?\n",
        "y_test=to_categorical(y_test, nb_class)"
      ],
      "id": "0727e39c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Budujemy prostą sieć neuronową posługując się podejściem funkcyjnym\n",
        "\n",
        "def build_simple_nn():\n",
        "    model=Sequential()\n",
        "    \n",
        "    #Warstwa wejsciowo/wyjsciowa\n",
        "    model.add(Dense(nb_class,input_shape=(reshaped,)))\n",
        "    \n",
        "    model.add(Activation(\"sigmoid\"))\n",
        "    model.summary()\n",
        "    model.compile(loss=[\"categorical_crossentropy\"], \n",
        "    metrics=[\"accuracy\"],optimizer=optimizer)\n",
        "    return model"
      ],
      "id": "3b38b799",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Istnieje możliwośc dodania funkcji aktywacji bezpośrednio w Dense layer za pomocą parametru `activation` lecz w ramach nauki dodajemy ją osobno aby pamiętać o strukturze budowania sieci\n"
      ],
      "id": "82b8d45a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model=build_simple_nn()"
      ],
      "id": "68d21330",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history=model.fit(X_train, y_train,\n",
        "batch_size=batch_size, epochs=nb_epochs,\n",
        "validation_split=val_split)"
      ],
      "id": "f46007ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "score=model.evaluate(X_test, y_test)"
      ],
      "id": "f979fdbd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Pokazujemy jak przebiegał trening naszej prostej sieci\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "id": "ab8d2515",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Brawo, właśnie wytrenowaliście i dokonaliście ewaluacji swojej pierwszej sieci neuronowej 🥳🥳🥳**\n",
        "\n",
        "## A możeby tak zrobić lepszą sieć?\n",
        "\n",
        "Czyli warstwy ukryte w akcji!\n"
      ],
      "id": "395f4b1c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#ustawmy nowy optymalizator dla nowej sieci, pozostala czesc parametrów\n",
        "#zostanie bez zmian\n",
        "\n",
        "optimizer_2=SGD()"
      ],
      "id": "4e8c00c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_nn():\n",
        "    model=Sequential()\n",
        "    \n",
        "    #Warstwa wejsciowa\n",
        "    model.add(Dense(nb_hidden, input_shape=(reshaped,)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    \n",
        "    #Warstwa ukryta\n",
        "    model.add(Dense(nb_hidden))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    \n",
        "    #Warstwa output\n",
        "    model.add(Dense(nb_class))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    model.summary()\n",
        "    model.compile(loss=[\"categorical_crossentropy\"],\n",
        "    metrics=[\"accuracy\"],optimizer=optimizer_2)\n",
        "    return model"
      ],
      "id": "f5250cc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_2=build_nn()"
      ],
      "id": "6c365fe1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history_2=model_2.fit(X_train,y_train, \n",
        "epochs=nb_epochs,\n",
        "validation_split=val_split, \n",
        "batch_size=batch_size)"
      ],
      "id": "bbce98aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "score_2=model_2.evaluate(X_test,y_test)"
      ],
      "id": "4d496075",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A jakby sieć zapominała część rzeczy jakich się nauczyła?\n",
        "\n",
        "Czyli co to takiego `Dropout` i dlaczego jest taki ważny?\n"
      ],
      "id": "ba7f2069"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras.layers import Dropout"
      ],
      "id": "7329e305",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer_3=SGD()\n",
        "drp=0.3"
      ],
      "id": "fe6130e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_nn_2():\n",
        "    model=Sequential()\n",
        "    \n",
        "    #Warstwa wejsciowa\n",
        "    model.add(Dense(nb_hidden,input_shape=(reshaped,)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(drp))\n",
        "    \n",
        "    #Warstwa ukryta\n",
        "    model.add(Dense(nb_hidden))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(drp))\n",
        "    \n",
        "    #Warstwa wyjsciowa\n",
        "    model.add(Dense(nb_class))\n",
        "    model.add(Activation(\"sigmoid\"))\n",
        "    model.summary()\n",
        "    model.compile(loss=[\"categorical_crossentropy\"],\n",
        "    optimizer=optimizer_3,metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "id": "9b4b7f3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_3=build_nn_2()"
      ],
      "id": "f91609b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history_3=model_3.fit(X_train, y_train, epochs=nb_epochs,\n",
        "validation_split=val_split, batch_size=batch_size)"
      ],
      "id": "efa02abd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "score_3=model_3.evaluate(X_test,y_test)"
      ],
      "id": "efb49926",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nowe optymalizatory + Dropout?\n",
        "\n",
        "Nie tylko samym `SGD` człowiek żyje, czyli `RMSprop` i `Adam` w praktyce 😄\n",
        "\n",
        "#### RMSprop\n"
      ],
      "id": "57bce706"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer_4=RMSprop()"
      ],
      "id": "e1a364fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_nn_with_new_optimizer():\n",
        "    model=Sequential()\n",
        "    \n",
        "    #Warstwa wejsciowa\n",
        "    model.add(Dense(nb_hidden, input_shape=(reshaped,)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(drp))\n",
        "    \n",
        "    #Warstwa ukryta\n",
        "    model.add(Dense(nb_hidden))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(drp))\n",
        "    \n",
        "    #Warstwa wyjsciowa\n",
        "    model.add(Dense(nb_class))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    model.summary()\n",
        "    model.compile(optimizer=optimizer_4, \n",
        "    loss=[\"categorical_crossentropy\"], metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "id": "c60ef615",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_4=build_nn_with_new_optimizer()"
      ],
      "id": "7556f7f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history_4=model_4.fit(X_train, y_train, \n",
        "epochs=nb_epochs, batch_size=batch_size,\n",
        "validation_split=val_split)"
      ],
      "id": "04f3c3cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "score_new_optimizer=model_4.evaluate(X_test,y_test)"
      ],
      "id": "4be0c931",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Adam\n"
      ],
      "id": "94fffabf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer_5=Adam()"
      ],
      "id": "ee701753",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_nn_with_new_optimizer_v2():\n",
        "    model=Sequential()\n",
        "    \n",
        "    #Warstwa wejsciowa\n",
        "    model.add(Dense(nb_hidden, input_shape=(reshaped,)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(drp))\n",
        "    \n",
        "    #Warstwa ukryta\n",
        "    model.add(Dense(nb_hidden))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(drp))\n",
        "    \n",
        "    #Warstwa wyjsciowa\n",
        "    model.add(Dense(nb_class))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    model.summary()\n",
        "    model.compile(optimizer=optimizer_5, \n",
        "    loss=[\"categorical_crossentropy\"], metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "id": "06017990",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_5=build_nn_with_new_optimizer_v2()"
      ],
      "id": "5f99d5c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history_5=model_5.fit(X_train, y_train, \n",
        "epochs=nb_epochs, batch_size=batch_size,\n",
        "validation_split=val_split)"
      ],
      "id": "b4268a52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "score_new_optimizer=model_5.evaluate(X_test,y_test)"
      ],
      "id": "33667a63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wykres jak przebiegał trening bo dawno nie było 😉\n"
      ],
      "id": "ecc5af21"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_5.history['accuracy'])\n",
        "plt.plot(history_5.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_5.history['loss'])\n",
        "plt.plot(history_5.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "id": "8b7cb950",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sekcja zagadek dla bystrzaków\n"
      ],
      "id": "02f9f1a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Co jest nie tak w tej sieci?\n",
        "#Dane wejsciowe są takie same, przeskalowany MNIST\n",
        "\n",
        "model_mistery=Sequential()\n",
        "    \n",
        "#Warstwa wejsciowa\n",
        "model_mistery.add(Dense(nb_hidden, input_shape=(reshaped,)))\n",
        "model_mistery.add(Activation(\"relu\"))\n",
        "model_mistery.add(Dropout(0.99))\n",
        "    \n",
        "#Warstwa ukryta\n",
        "model_mistery.add(Dense(nb_hidden))\n",
        "model_mistery.add(Activation(\"relu\"))\n",
        "model_mistery.add(Dropout(0.99))\n",
        "    \n",
        "#Warstwa wyjsciowa\n",
        "model_mistery.add(Dense(nb_class))\n",
        "model_mistery.add(Activation(\"softmax\"))\n",
        "\n",
        "model_mistery.summary()\n",
        "model_mistery.compile(optimizer=optimizer_5, \n",
        "loss=[\"categorical_crossentropy\"], metrics=[\"accuracy\"])"
      ],
      "id": "2bbac614",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history_mistery=history_5=model_5.fit(X_train, y_train, \n",
        "epochs=nb_epochs, batch_size=batch_size,\n",
        "validation_split=val_split)"
      ],
      "id": "effcf404",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "score_mistery=model_mistery.evaluate(X_test,y_test)"
      ],
      "id": "d164cfc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](Squish_it_mw.mp4)"
      ],
      "id": "22e87f1e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}