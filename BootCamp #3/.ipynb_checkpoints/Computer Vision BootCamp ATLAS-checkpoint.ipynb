{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"\"\n",
        "author: \"\"\n",
        "format: html\n",
        "editor: visual\n",
        "---"
      ],
      "id": "7f4781db"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BootCamp ATLAS\n",
        "\n",
        "***Author : Mateusz Walo***\n",
        "\n",
        "# Wstp do uczenia gbokiego\n",
        "\n",
        "Czym jest uczenie gbokie i jak ma si w stosunku do klasycznego uczenia maszynowego kt贸re poznae na poprzednich zajciach?\n",
        "\n",
        "![](mldl.png)\n",
        "\n",
        "## Sieci neuronowe - idea\n",
        "\n",
        "Czym s sieci neuronowe? Z czego si skadaj? Dlaczego s tak istotne w wizji komputerowej?\n",
        "\n",
        "![](nn_image_bootcamp.png){fig-alt=\"Model sieci neuronowej\"}\n",
        "\n",
        "## Perceptron\n",
        "\n",
        "![](Perceptron.png)\n",
        "\n",
        "Prosty algorytm kt贸ry dla wektora `x` o `n` wartociach, `x=(x1,x2,x3,...xn)` nazywanych w przyszoci cechami *(ang. features)* zwraca wynik 0 (fasz) albo 1 (prawda)\n",
        "\n",
        "$$\n",
        "f(x) = \n",
        "\\begin{cases} \n",
        "1 & \\text{jeli } wx + b > 0 \\\\\n",
        "0 & \\text{w ka偶dym innym przypadku}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\\\n",
        "Gdzie `b` symbolizuje ***bias***, `w` jest wektorem wag a `wx` jest wynikiem sumy\n",
        "\n",
        "$$\n",
        "wx=\\sum_{j=1}^{m} w_j x_j\n",
        "$$\\\n",
        "\n",
        "***Bias** - Dodatkowy parametr dodawany do sumy wa偶onej w ka偶dym neuronie, przesuwa on `funkcj aktywacji` w taki spos贸b aby sie moga dopasowa si do danych*\n",
        "\n",
        "**Spostrze偶enie:** Jak pamitacie z Geometrii Analitycznej `wx+b` stanowi hiperpaszczyzne graniczn kt贸ra zmienia swoje poo偶enie w przestrzeni (w tym wypadku n-wymiarowej) w zale偶noci od `w` i `b` \n",
        "\n",
        "## Dlaczego Perceptron jest problematyczny w treningu?\n",
        "\n",
        "We偶my pod uwag pojedyczy neuron, jak wybra najlepsze wagi i bias?\n",
        "\n",
        "W og贸lbym przypadku chcemy dostarczy zbi贸r treningowy aby model sam ustali wag i bias, tak aby minimalizowa liczb bd贸w wraz z kolejnymi epokami sieci. Wyobra藕my sobie 偶e mamy zbi贸r zdj 偶yraf (takie du偶e dugoszyjne zwierze afrykaskie) i zbi贸r kt贸ry nie zawiera ani jednej 偶yrafy, dokonajmy zao偶enia 偶e ka偶dy neuron wejciowy przyjmuje dane z jednego piksela na zdjciu. Podczas gdy komputer przetwarza te obrazy, chcielibymy, aby nasz neuron dostosowywa swoje wagi i bias, abymy mieli coraz mniej obraz贸w bdnie rozpoznawanych jako stworzenia bdce nie-偶yrafami. Wszystko byoby piknie gdyby niewielka zmiana wartoci `w` i `b` powodowaa tylko niewielk zmian w predykcji sieci. Natomiast mamy du偶y skok wyjciowy, co uniemo偶liwia nam progresywne uczenie sieci. Co prezentuje wykres poni偶ej\n",
        "\n",
        "![](pppp.png){fig-align=\"center\" width=\"500\"}\n",
        "\n",
        "Matematycy obecni na sali z atwoci zauwa偶 偶e dziej si tak poniewa偶 ta funkcja nie jest ciga w zerze \n",
        "\n",
        "## Ratunek - funkcje aktywacyjne\n",
        "\n",
        "### \n",
        "\n",
        "***Sigmoid*** - tak nazywamy funkcj aktywacyjn dan wzorem\n",
        "\n",
        "$$\n",
        "\\sigma(x)=\\dfrac{1}{1+e^{-x}}\n",
        "$$\n",
        "\n",
        "Jej wykres prezentuje si nastpujco\n",
        "\n",
        "![Jak widzimy w tym wypadku funkcja jest ciga w zerze, co pozwala sieci na odpowied藕 \"mo偶e\" gdy nie jest pewna swojej predykcji w trudniejszych przypadkach. Jak dobrze pamitacie z algebry liniowej istnieje co takiego jak kombinacja liniowa, bez funkcji aktywacji wagi i bias przekazywane do nastepnych warstw bylyby tyko kombinacj liniow, nie zmieniayby swoich wartoci i siec nie mogaby si uczy.](Logistic-curve.png){fig-align=\"center\"}\n",
        "\n",
        "**ReLU -**tak nazywamy funkcj dan wzorem\n",
        "\n",
        "$$\n",
        "f(x)=max(0,x)\n",
        "$$\n",
        "\n",
        "kt贸rej wykres prezentuje si nastpujco\n",
        "\n",
        "![](relu.png){fig-align=\"center\"}\n",
        "\n",
        "***Dla dociekliwych:** Istnieje kilka innych funkcji aktywacyjnych, kt贸re om贸wimy szczeg贸owo w dalszej czci BootCampu *\n",
        "\n",
        "## Funkcje straty\n",
        "\n",
        "**MSE** - To jest redni bd kwadratowy midzy przewidywaniami a prawdziwymi wartociami. Matematycznie, jeli `Y'` jest wektorem `n` predykcji, a `Y` jest wektorem `n` obserwowanych wartoci, to speniaj one nastpujce r贸wnanie:\n",
        "\n",
        "$$\n",
        "MSE=\\dfrac{1}{n}\\sum_{i=1}^{n}(Y'-Y)^2\n",
        "$$\n",
        "\n",
        "**Spostrze偶enie:** *Dziki tej funkcji mo偶emy dostrzec 偶e jeli predykcje bd si r贸偶ni znaczco od wartoci prawdziwych, to roznica bedzie widocza drastycznie, zobaczymy ten bd do kwadratu!*\n",
        "\n",
        "**Binary cross-entropy** - Jest to binarna logarytmiczna strata. Za贸偶my, 偶e predykcja modelu to `p`, podczas gdy naszym targetem jest `t` w贸wczas binarna entropia krzy偶owa jest zdefiniowana nastpujco:\n",
        "\n",
        "$$\n",
        "-tln(p)-(1-t)ln(1-p)\n",
        "$$\n",
        "\n",
        "**Spostrze偶enie:** Wykorzystywana najczciej do klasyfikacji binarnej (na studiach bdziecie jej u偶ywa najczciej )\n",
        "\n",
        "**Categorical cross-entropy -** oznaczenia podobnie jak wy偶ej\n",
        "\n",
        "$$\n",
        "L_i=-\\sum jt_{i,j}ln(p_{i,j})\n",
        "$$\n",
        "\n",
        "**Spostrze偶enie:** Wykorzystywana najczciej do klasyfikacji wieloetykietowej (zaraz zobaczycie j w akcji)\n",
        "\n",
        "## Optymalizator\n",
        "\n",
        "Wyobra藕my sobie og贸ln funkcj kosztu C(w) jednej zmiennej jak na poni偶szym wykresie:\n",
        "\n",
        "![](opt.png){fig-align=\"center\"}\n",
        "\n",
        "Schodzenie po gradiencie mo偶na postrzega jako wdrowca, kt贸ry zamierza zej z g贸ry do doliny. G贸ra reprezentuje funkcj `C`, podczas gdy dolina reprezentuje minimalne `min` . Wdrowiec ma punkt pocztkowy `w0` . Wdrowiec porusza si powoli. Na ka偶dym kroku `r` gradient jest kierunkiem maksymalnego wzrostu. Matematycznie ten kierunek jest wartoci pochodnej czstkowej w punkcie `wr` osignitym w kroku `r`. Dlatego te偶, wybierajc przeciwny kierunek, wdrowiec mo偶e porusza si w kierunku doliny. Na ka偶dym kroku wdrowiec mo偶e decydowa, jak odlegoc chce pokona nastpnym krokiem. Jest to tempo uczenia si. Nale偶y zauwa偶y, 偶e jeli jest zbyt mae, wdrowiec bdzie porusza si powoli. Jednak jeli jest zbyt du偶e, wdrowiec prawdopodobnie ominie dolin.\n",
        "\n",
        "## Realny przykad - Baza MNIST\n",
        "\n",
        "Baza danych MNIST zawiera 70 000 zdjc odrcznie pisanych cyfr, w skali szaroci, ka偶de wymiaru 28x28 pikseli.\n",
        "\n",
        "Jak zaimportowa tensorflow i MNIST?\n"
      ],
      "id": "f16543ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -U numpy --quiet"
      ],
      "id": "19ad684e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!pip install -U tensorflow pandas matplotlib --quiet\n",
        "#Instrukcja powy偶ej jeli nie masz zainstalowanych bibliotek\n",
        "\n",
        "#Importujemy potrzebne biblioteki\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "c74b5445",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Ustalamy warto podstawowych parametr贸w\n",
        "\n",
        "nb_epochs=10 #Liczba epok przez kt贸re bdzie si trenowa sie\n",
        "val_split=0.2 #Czesc jaka zostanie przydzielona na zbior walidacyjny\n",
        "nb_class=10 #Liczba klas output贸w, cyfry of 0 do 9\n",
        "nb_hidden=128 #Liczba warstw ukrytych\n",
        "reshaped=784 #do skalowania zdj, 784=28*28 co odpowiada wymiarowi zdjc\n",
        "batch_size=128 #Liczba pr贸bek kt贸ra bdzie przetwarzana w 1 obrocie\n",
        "optimizer=SGD() #Optymalizator Stochastic gradient descent"
      ],
      "id": "362c4c3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Wczytujemy dane mnist\n",
        "\n",
        "(X_train, y_train),(X_test, y_test)=mnist.load_data()"
      ],
      "id": "426547a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Sprawdzmy czy dane wczytay si poprawnie\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(X_train[1])\n",
        "plt.title(f\"Na rysunku znajduj si cyfra {y_train[1]}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "3e8258ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Przeksztacamy, normalizujemy dane mnist\n",
        "\n",
        "X_train=X_train.reshape(60000, reshaped)\n",
        "X_test=X_test.reshape(10000, reshaped)\n",
        "X_train=X_train.astype(\"float32\")/255\n",
        "X_test=X_test.astype(\"float32\")/255\n",
        "y_train=to_categorical(y_train, nb_class) #Pamitacie co to OneHotEncoding ;)?\n",
        "y_test=to_categorical(y_test, nb_class)"
      ],
      "id": "d88aa51e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Budujemy prost sie neuronow posugujc si podejciem funkcyjnym\n",
        "\n",
        "def build_simple_nn():\n",
        "    model=Sequential()\n",
        "    \n",
        "    #Warstwa wejsciowo/wyjsciowa\n",
        "    model.add(Dense(nb_class,input_shape=(reshaped,)))\n",
        "    \n",
        "    model.add(Activation(\"sigmoid\"))\n",
        "    model.summary()\n",
        "    model.compile(loss=[\"categorical_crossentropy\"], \n",
        "    metrics=[\"accuracy\"],optimizer=optimizer)\n",
        "    return model"
      ],
      "id": "369b9488",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Istnieje mo偶liwoc dodania funkcji aktywacji bezporednio w Dense layer za pomoc parametru `activation` lecz w ramach nauki dodajemy j osobno aby pamita o strukturze budowania sieci\n"
      ],
      "id": "c38c2567"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model=build_simple_nn()"
      ],
      "id": "c5b1c70e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history=model.fit(X_train, y_train,\n",
        "batch_size=batch_size, epochs=nb_epochs,\n",
        "validation_split=val_split)"
      ],
      "id": "7d3a880e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "score=model.evaluate(X_test, y_test)"
      ],
      "id": "1049fd27",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Pokazujemy jak przebiega trening naszej prostej sieci\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "id": "6b96c3d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Brawo, wanie wytrenowalicie i dokonalicie ewaluacji swojej pierwszej sieci neuronowej コココ**\n",
        "\n",
        "## A mo偶eby tak zrobi lepsz sie?\n",
        "\n",
        "Czyli warstwy ukryte w akcji!\n"
      ],
      "id": "6a63bf15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#ustawmy nowy optymalizator dla nowej sieci, pozostala czesc parametr贸w\n",
        "#zostanie bez zmian\n",
        "\n",
        "optimizer_2=SGD()"
      ],
      "id": "f314555a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_nn():\n",
        "    model=Sequential()\n",
        "    \n",
        "    #Warstwa wejsciowa\n",
        "    model.add(Dense(nb_hidden, input_shape=(reshaped,)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    \n",
        "    #Warstwa ukryta\n",
        "    model.add(Dense(nb_hidden))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    \n",
        "    #Warstwa output\n",
        "    model.add(Dense(nb_class))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    model.summary()\n",
        "    model.compile(loss=[\"categorical_crossentropy\"],\n",
        "    metrics=[\"accuracy\"],optimizer=optimizer_2)\n",
        "    return model"
      ],
      "id": "9ccadc85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_2=build_nn()"
      ],
      "id": "6cea1a5d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history_2=model_2.fit(X_train,y_train, \n",
        "epochs=nb_epochs,\n",
        "validation_split=val_split, \n",
        "batch_size=batch_size)"
      ],
      "id": "308bd2af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "score_2=model_2.evaluate(X_test,y_test)"
      ],
      "id": "f0fe071b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A jakby sie zapominaa cz rzeczy jakich si nauczya?\n",
        "\n",
        "Czyli co to takiego `Dropout` i dlaczego jest taki wa偶ny?\n"
      ],
      "id": "36cac2ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras.layers import Dropout"
      ],
      "id": "a9e0ba90",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer_3=SGD()\n",
        "drp=0.3"
      ],
      "id": "bc37ad59",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_nn_2():\n",
        "    model=Sequential()\n",
        "    \n",
        "    #Warstwa wejsciowa\n",
        "    model.add(Dense(nb_hidden,input_shape=(reshaped,)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(drp))\n",
        "    \n",
        "    #Warstwa ukryta\n",
        "    model.add(Dense(nb_hidden))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(drp))\n",
        "    \n",
        "    #Warstwa wyjsciowa\n",
        "    model.add(Dense(nb_class))\n",
        "    model.add(Activation(\"sigmoid\"))\n",
        "    model.summary()\n",
        "    model.compile(loss=[\"categorical_crossentropy\"],\n",
        "    optimizer=optimizer_3,metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "id": "146b27fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_3=build_nn_2()"
      ],
      "id": "9185c323",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history_3=model_3.fit(X_train, y_train, epochs=nb_epochs,\n",
        "validation_split=val_split, batch_size=batch_size)"
      ],
      "id": "ef9094ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "score_3=model_3.evaluate(X_test,y_test)"
      ],
      "id": "79b6ab61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nowe optymalizatory + Dropout?\n",
        "\n",
        "Nie tylko samym `SGD` czowiek 偶yje, czyli `RMSprop` i `Adam` w praktyce \n",
        "\n",
        "#### RMSprop\n"
      ],
      "id": "a347a178"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer_4=RMSprop()"
      ],
      "id": "970b316f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_nn_with_new_optimizer():\n",
        "    model=Sequential()\n",
        "    \n",
        "    #Warstwa wejsciowa\n",
        "    model.add(Dense(nb_hidden, input_shape=(reshaped,)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(drp))\n",
        "    \n",
        "    #Warstwa ukryta\n",
        "    model.add(Dense(nb_hidden))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(drp))\n",
        "    \n",
        "    #Warstwa wyjsciowa\n",
        "    model.add(Dense(nb_class))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    model.summary()\n",
        "    model.compile(optimizer=optimizer_4, \n",
        "    loss=[\"categorical_crossentropy\"], metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "id": "6e070667",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_4=build_nn_with_new_optimizer()"
      ],
      "id": "be2a54ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history_4=model_4.fit(X_train, y_train, \n",
        "epochs=nb_epochs, batch_size=batch_size,\n",
        "validation_split=val_split)"
      ],
      "id": "601a33c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "score_new_optimizer=model_4.evaluate(X_test,y_test)"
      ],
      "id": "7224e7b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Adam\n"
      ],
      "id": "088f8c9a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer_5=Adam()"
      ],
      "id": "75546359",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_nn_with_new_optimizer_v2():\n",
        "    model=Sequential()\n",
        "    \n",
        "    #Warstwa wejsciowa\n",
        "    model.add(Dense(nb_hidden, input_shape=(reshaped,)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(drp))\n",
        "    \n",
        "    #Warstwa ukryta\n",
        "    model.add(Dense(nb_hidden))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(drp))\n",
        "    \n",
        "    #Warstwa wyjsciowa\n",
        "    model.add(Dense(nb_class))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    model.summary()\n",
        "    model.compile(optimizer=optimizer_5, \n",
        "    loss=[\"categorical_crossentropy\"], metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "id": "bd39e8ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_5=build_nn_with_new_optimizer_v2()"
      ],
      "id": "08d88b50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history_5=model_5.fit(X_train, y_train, \n",
        "epochs=nb_epochs, batch_size=batch_size,\n",
        "validation_split=val_split)"
      ],
      "id": "acc82a89",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "score_new_optimizer=model_5.evaluate(X_test,y_test)"
      ],
      "id": "791fd295",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wykres jak przebiega trening bo dawno nie byo \n"
      ],
      "id": "6926abf1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_5.history['accuracy'])\n",
        "plt.plot(history_5.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_5.history['loss'])\n",
        "plt.plot(history_5.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "id": "173ec868",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sekcja zagadek dla bystrzak贸w\n"
      ],
      "id": "40f1b9ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Co jest nie tak w tej sieci?\n",
        "#Dane wejsciowe s takie same, przeskalowany MNIST\n",
        "\n",
        "model_mistery=Sequential()\n",
        "    \n",
        "#Warstwa wejsciowa\n",
        "model_mistery.add(Dense(nb_hidden, input_shape=(reshaped,)))\n",
        "model_mistery.add(Activation(\"relu\"))\n",
        "model_mistery.add(Dropout(0.99))\n",
        "    \n",
        "#Warstwa ukryta\n",
        "model_mistery.add(Dense(nb_hidden))\n",
        "model_mistery.add(Activation(\"relu\"))\n",
        "model_mistery.add(Dropout(0.99))\n",
        "    \n",
        "#Warstwa wyjsciowa\n",
        "model_mistery.add(Dense(nb_class))\n",
        "model_mistery.add(Activation(\"softmax\"))\n",
        "\n",
        "model_mistery.summary()\n",
        "model_mistery.compile(optimizer=optimizer_5, \n",
        "loss=[\"categorical_crossentropy\"], metrics=[\"accuracy\"])"
      ],
      "id": "3aa7f0da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history_mistery=history_5=model_5.fit(X_train, y_train, \n",
        "epochs=nb_epochs, batch_size=batch_size,\n",
        "validation_split=val_split)"
      ],
      "id": "5eeabbac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "score_mistery=model_mistery.evaluate(X_test,y_test)"
      ],
      "id": "67a16537",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!pip install jupyter --quiet"
      ],
      "id": "bb4035cf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}