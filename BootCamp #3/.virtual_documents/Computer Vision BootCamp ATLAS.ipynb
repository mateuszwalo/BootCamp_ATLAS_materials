


!pip install -U numpy --quiet


#!pip install -U tensorflow pandas matplotlib --quiet
#Instrukcja powyżej jeśli nie masz zainstalowanych bibliotek

#Importujemy potrzebne biblioteki

from tensorflow.keras.models import Sequential
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.utils import to_categorical
import numpy as np 
import matplotlib.pyplot as plt


#Ustalamy wartość podstawowych parametrów

nb_epochs=10 #Liczba epok przez które będzie się trenować sieć
val_split=0.2 #Czesc jaka zostanie przydzielona na zbior walidacyjny
nb_class=10 #Liczba klas outputów, cyfry of 0 do 9
nb_hidden=128 #Liczba warstw ukrytych
reshaped=784 #do skalowania zdjęć, 784=28*28 co odpowiada wymiarowi zdjęc
batch_size=128 #Liczba próbek która będzie przetwarzana w 1 obrocie
optimizer=SGD() #Optymalizator Stochastic gradient descent


#Wczytujemy dane mnist

(X_train, y_train),(X_test, y_test)=mnist.load_data()


#Sprawdzmy czy dane wczytały się poprawnie

plt.figure()
plt.imshow(X_train[1])
plt.title(f"Na rysunku znajduję się cyfra {y_train[1]}")
plt.tight_layout()
plt.show()


# Przekształcamy, normalizujemy dane mnist

X_train=X_train.reshape(60000, reshaped)
X_test=X_test.reshape(10000, reshaped)
X_train=X_train.astype("float32")/255
X_test=X_test.astype("float32")/255
y_train=to_categorical(y_train, nb_class) #Pamiętacie co to OneHotEncoding ;)?
y_test=to_categorical(y_test, nb_class)


#Budujemy prostą sieć neuronową posługując się podejściem funkcyjnym

def build_simple_nn():
    model=Sequential()
    
    #Warstwa wejsciowo/wyjsciowa
    model.add(Dense(nb_class,input_shape=(reshaped,)))
    
    model.add(Activation("sigmoid"))
    model.summary()
    model.compile(loss=["categorical_crossentropy"], 
    metrics=["accuracy"],optimizer=optimizer)
    return model





model=build_simple_nn()


history=model.fit(X_train, y_train,
batch_size=batch_size, epochs=nb_epochs,
validation_split=val_split)


score=model.evaluate(X_test, y_test)


#Pokazujemy jak przebiegał trening naszej prostej sieci
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')

plt.show()





#ustawmy nowy optymalizator dla nowej sieci, pozostala czesc parametrów
#zostanie bez zmian

optimizer_2=SGD()


def build_nn():
    model=Sequential()
    
    #Warstwa wejsciowa
    model.add(Dense(nb_hidden, input_shape=(reshaped,)))
    model.add(Activation("relu"))
    
    #Warstwa ukryta
    model.add(Dense(nb_hidden))
    model.add(Activation("relu"))
    
    #Warstwa output
    model.add(Dense(nb_class))
    model.add(Activation("softmax"))
    model.summary()
    model.compile(loss=["categorical_crossentropy"],
    metrics=["accuracy"],optimizer=optimizer_2)
    return model


model_2=build_nn()


history_2=model_2.fit(X_train,y_train, 
epochs=nb_epochs,
validation_split=val_split, 
batch_size=batch_size)


score_2=model_2.evaluate(X_test,y_test)





from tensorflow.keras.layers import Dropout


optimizer_3=SGD()
drp=0.3


def build_nn_2():
    model=Sequential()
    
    #Warstwa wejsciowa
    model.add(Dense(nb_hidden,input_shape=(reshaped,)))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa ukryta
    model.add(Dense(nb_hidden))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa wyjsciowa
    model.add(Dense(nb_class))
    model.add(Activation("sigmoid"))
    model.summary()
    model.compile(loss=["categorical_crossentropy"],
    optimizer=optimizer_3,metrics=["accuracy"])
    return model


model_3=build_nn_2()


history_3=model_3.fit(X_train, y_train, epochs=nb_epochs,
validation_split=val_split, batch_size=batch_size)


score_3=model_3.evaluate(X_test,y_test)





from tensorflow.keras.optimizers import RMSprop

optimizer_4=RMSprop()


def build_nn_with_new_optimizer():
    model=Sequential()
    
    #Warstwa wejsciowa
    model.add(Dense(nb_hidden, input_shape=(reshaped,)))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa ukryta
    model.add(Dense(nb_hidden))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa wyjsciowa
    model.add(Dense(nb_class))
    model.add(Activation("softmax"))
    model.summary()
    model.compile(optimizer=optimizer_4, 
    loss=["categorical_crossentropy"], metrics=["accuracy"])
    return model


model_4=build_nn_with_new_optimizer()


history_4=model_4.fit(X_train, y_train, 
epochs=nb_epochs, batch_size=batch_size,
validation_split=val_split)


score_new_optimizer=model_4.evaluate(X_test,y_test)





from tensorflow.keras.optimizers import Adam

optimizer_5=Adam()


def build_nn_with_new_optimizer_v2():
    model=Sequential()
    
    #Warstwa wejsciowa
    model.add(Dense(nb_hidden, input_shape=(reshaped,)))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa ukryta
    model.add(Dense(nb_hidden))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa wyjsciowa
    model.add(Dense(nb_class))
    model.add(Activation("softmax"))
    model.summary()
    model.compile(optimizer=optimizer_5, 
    loss=["categorical_crossentropy"], metrics=["accuracy"])
    return model


model_5=build_nn_with_new_optimizer_v2()


history_5=model_5.fit(X_train, y_train, 
epochs=nb_epochs, batch_size=batch_size,
validation_split=val_split)


score_new_optimizer=model_5.evaluate(X_test,y_test)





plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history_5.history['accuracy'])
plt.plot(history_5.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(history_5.history['loss'])
plt.plot(history_5.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')

plt.show()





#Co jest nie tak w tej sieci?
#Dane wejsciowe są takie same, przeskalowany MNIST

model_mistery=Sequential()
    
#Warstwa wejsciowa
model_mistery.add(Dense(nb_hidden, input_shape=(reshaped,)))
model_mistery.add(Activation("relu"))
model_mistery.add(Dropout(0.99))
    
#Warstwa ukryta
model_mistery.add(Dense(nb_hidden))
model_mistery.add(Activation("relu"))
model_mistery.add(Dropout(0.99))
    
#Warstwa wyjsciowa
model_mistery.add(Dense(nb_class))
model_mistery.add(Activation("softmax"))

model_mistery.summary()
model_mistery.compile(optimizer=optimizer_5, 
loss=["categorical_crossentropy"], metrics=["accuracy"])


history_mistery=history_5=model_5.fit(X_train, y_train, 
epochs=nb_epochs, batch_size=batch_size,
validation_split=val_split)


score_mistery=model_mistery.evaluate(X_test,y_test)
