---
title: ""
author: ""
format: html
editor: visual
---

# BootCamp ATLAS #3

***Author : Mateusz Walo***

# Wstp do uczenia gbokiego

Czym jest uczenie gbokie i jak ma si w stosunku do klasycznego uczenia maszynowego kt贸re poznae na poprzednich zajciach?

![](mldl.png)

## Sieci neuronowe - idea

Czym s sieci neuronowe? Z czego si skadaj? Dlaczego s tak istotne w wizji komputerowej?

![](nn_image_bootcamp.png){fig-alt="Model sieci neuronowej"}

## Perceptron

![](Perceptron.png)

Prosty algorytm kt贸ry dla wektora `x` o `n` wartociach, `x=(x1,x2,x3,...xn)` nazywanych w przyszoci cechami *(ang. features)* zwraca wynik 0 (fasz) albo 1 (prawda)

$$
f(x) = 
\begin{cases} 
1 & \text{jeli } wx + b > 0 \\
0 & \text{w ka偶dym innym przypadku}
\end{cases}
$$

\
Gdzie `b` symbolizuje ***bias***, `w` jest wektorem wag a `wx` jest wynikiem sumy

$$
wx=\sum_{j=1}^{m} w_j x_j
$$\

***Bias** - Dodatkowy parametr dodawany do sumy wa偶onej w ka偶dym neuronie, przesuwa on `funkcj aktywacji` w taki spos贸b aby sie moga dopasowa si do danych*

**Spostrze偶enie:** Jak pamitacie z Geometrii Analitycznej `wx+b` stanowi hiperpaszczyzne graniczn kt贸ra zmienia swoje poo偶enie w przestrzeni (w tym wypadku n-wymiarowej) w zale偶noci od `w` i `b` 

## Dlaczego Perceptron jest problematyczny w treningu?

We偶my pod uwag pojedyczy neuron, jak wybra najlepsze wagi i bias?

W og贸lbym przypadku chcemy dostarczy zbi贸r treningowy aby model sam ustali wag i bias, tak aby minimalizowa liczb bd贸w wraz z kolejnymi epokami sieci. Wyobra藕my sobie 偶e mamy zbi贸r zdj 偶yraf (takie du偶e dugoszyjne zwierze afrykaskie) i zbi贸r kt贸ry nie zawiera ani jednej 偶yrafy, dokonajmy zao偶enia 偶e ka偶dy neuron wejciowy przyjmuje dane z jednego piksela na zdjciu. Podczas gdy komputer przetwarza te obrazy, chcielibymy, aby nasz neuron dostosowywa swoje wagi i bias, abymy mieli coraz mniej obraz贸w bdnie rozpoznawanych jako stworzenia bdce nie-偶yrafami. Wszystko byoby piknie gdyby niewielka zmiana wartoci `w` i `b` powodowaa tylko niewielk zmian w predykcji sieci. Natomiast mamy du偶y skok wyjciowy, co uniemo偶liwia nam progresywne uczenie sieci. Co prezentuje wykres poni偶ej

![](pppp.png){fig-align="center" width="500"}

Matematycy obecni na sali z atwoci zauwa偶 偶e dziej si tak poniewa偶 ta funkcja nie jest ciga w zerze 

## Ratunek - funkcje aktywacyjne

### 

***Sigmoid*** - tak nazywamy funkcj aktywacyjn dan wzorem

$$
\sigma(x)=\dfrac{1}{1+e^{-x}}
$$

Jej wykres prezentuje si nastpujco

![Jak widzimy w tym wypadku funkcja jest ciga w zerze, co pozwala sieci na odpowied藕 "mo偶e" gdy nie jest pewna swojej predykcji w trudniejszych przypadkach. Jak dobrze pamitacie z algebry liniowej istnieje co takiego jak kombinacja liniowa, bez funkcji aktywacji wagi i bias przekazywane do nastepnych warstw bylyby tyko kombinacj liniow, nie zmieniayby swoich wartoci i siec nie mogaby si uczy.](Logistic-curve.png){fig-align="center"}

**ReLU -**tak nazywamy funkcj dan wzorem

$$
f(x)=max(0,x)
$$

kt贸rej wykres prezentuje si nastpujco

![](relu.png){fig-align="center"}

***Dla dociekliwych:** Istnieje kilka innych funkcji aktywacyjnych, kt贸re om贸wimy szczeg贸owo w dalszej czci BootCampu *

## Funkcje straty

**MSE** - To jest redni bd kwadratowy midzy przewidywaniami a prawdziwymi wartociami. Matematycznie, jeli `Y'` jest wektorem `n` predykcji, a `Y` jest wektorem `n` obserwowanych wartoci, to speniaj one nastpujce r贸wnanie:

$$
MSE=\dfrac{1}{n}\sum_{i=1}^{n}(Y'-Y)^2
$$

**Spostrze偶enie:** *Dziki tej funkcji mo偶emy dostrzec 偶e jeli predykcje bd si r贸偶ni znaczco od wartoci prawdziwych, to roznica bedzie widocza drastycznie, zobaczymy ten bd do kwadratu!*

**Binary cross-entropy** - Jest to binarna logarytmiczna strata. Za贸偶my, 偶e predykcja modelu to `p`, podczas gdy naszym targetem jest `t` w贸wczas binarna entropia krzy偶owa jest zdefiniowana nastpujco:

$$
-tln(p)-(1-t)ln(1-p)
$$

**Spostrze偶enie:** Wykorzystywana najczciej do klasyfikacji binarnej (na studiach bdziecie jej u偶ywa najczciej )

**Categorical cross-entropy -** oznaczenia podobnie jak wy偶ej

$$
L_i=-\sum jt_{i,j}ln(p_{i,j})
$$

**Spostrze偶enie:** Wykorzystywana najczciej do klasyfikacji wieloetykietowej (zaraz zobaczycie j w akcji)

## Optymalizator

Wyobra藕my sobie og贸ln funkcj kosztu C(w) jednej zmiennej jak na poni偶szym wykresie:

![](opt.png){fig-align="center"}

Schodzenie po gradiencie mo偶na postrzega jako wdrowca, kt贸ry zamierza zej z g贸ry do doliny. G贸ra reprezentuje funkcj `C`, podczas gdy dolina reprezentuje minimalne `min` . Wdrowiec ma punkt pocztkowy `w0` . Wdrowiec porusza si powoli. Na ka偶dym kroku `r` gradient jest kierunkiem maksymalnego wzrostu. Matematycznie ten kierunek jest wartoci pochodnej czstkowej w punkcie `wr` osignitym w kroku `r`. Dlatego te偶, wybierajc przeciwny kierunek, wdrowiec mo偶e porusza si w kierunku doliny. Na ka偶dym kroku wdrowiec mo偶e decydowa, jak odlegoc chce pokona nastpnym krokiem. Jest to tempo uczenia si. Nale偶y zauwa偶y, 偶e jeli jest zbyt mae, wdrowiec bdzie porusza si powoli. Jednak jeli jest zbyt du偶e, wdrowiec prawdopodobnie ominie dolin.

## Realny przykad - Baza MNIST

Baza danych MNIST zawiera 70 000 zdjc odrcznie pisanych cyfr, w skali szaroci, ka偶de wymiaru 28x28 pikseli.

Jak zaimportowa tensorflow i MNIST?

```{python}
!pip install -U numpy --quiet
```

```{python}
#!pip install -U tensorflow pandas matplotlib --quiet
#Instrukcja powy偶ej jeli nie masz zainstalowanych bibliotek

#Importujemy potrzebne biblioteki

from tensorflow.keras.models import Sequential
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.utils import to_categorical
import numpy as np 
import matplotlib.pyplot as plt
```

```{python}
#Ustalamy warto podstawowych parametr贸w

nb_epochs=10 #Liczba epok przez kt贸re bdzie si trenowa sie
val_split=0.2 #Czesc jaka zostanie przydzielona na zbior walidacyjny
nb_class=10 #Liczba klas output贸w, cyfry of 0 do 9
nb_hidden=128 #Liczba warstw ukrytych
reshaped=784 #do skalowania zdj, 784=28*28 co odpowiada wymiarowi zdjc
batch_size=128 #Liczba pr贸bek kt贸ra bdzie przetwarzana w 1 obrocie
optimizer=SGD() #Optymalizator Stochastic gradient descent
```

```{python}
#Wczytujemy dane mnist

(X_train, y_train),(X_test, y_test)=mnist.load_data()
```

```{python}
#Sprawdzmy czy dane wczytay si poprawnie

plt.figure()
plt.imshow(X_train[1])
plt.title(f"Na rysunku znajduj si cyfra {y_train[1]}")
plt.tight_layout()
plt.show()
```

```{python}
# Przeksztacamy, normalizujemy dane mnist

X_train=X_train.reshape(60000, reshaped)
X_test=X_test.reshape(10000, reshaped)
X_train=X_train.astype("float32")/255
X_test=X_test.astype("float32")/255
y_train=to_categorical(y_train, nb_class) #Pamitacie co to OneHotEncoding ;)?
y_test=to_categorical(y_test, nb_class)
```

```{python}
#Budujemy prost sie neuronow posugujc si podejciem funkcyjnym

def build_simple_nn():
    model=Sequential()
    
    #Warstwa wejsciowo/wyjsciowa
    model.add(Dense(nb_class,input_shape=(reshaped,)))
    
    model.add(Activation("sigmoid"))
    model.summary()
    model.compile(loss=["categorical_crossentropy"], 
    metrics=["accuracy"],optimizer=optimizer)
    return model
```

-   Istnieje mo偶liwoc dodania funkcji aktywacji bezporednio w Dense layer za pomoc parametru `activation` lecz w ramach nauki dodajemy j osobno aby pamita o strukturze budowania sieci

```{python}
model=build_simple_nn()

```

```{python}
history=model.fit(X_train, y_train,
batch_size=batch_size, epochs=nb_epochs,
validation_split=val_split)
```

```{python}

score=model.evaluate(X_test, y_test)
```

```{python}
#Pokazujemy jak przebiega trening naszej prostej sieci
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')

plt.show()
```

**Brawo, wanie wytrenowalicie i dokonalicie ewaluacji swojej pierwszej sieci neuronowej コココ**

## A mo偶eby tak zrobi lepsz sie?

Czyli warstwy ukryte w akcji!

```{python}
#ustawmy nowy optymalizator dla nowej sieci, pozostala czesc parametr贸w
#zostanie bez zmian

optimizer_2=SGD()
```

```{python}
def build_nn():
    model=Sequential()
    
    #Warstwa wejsciowa
    model.add(Dense(nb_hidden, input_shape=(reshaped,)))
    model.add(Activation("relu"))
    
    #Warstwa ukryta
    model.add(Dense(nb_hidden))
    model.add(Activation("relu"))
    
    #Warstwa output
    model.add(Dense(nb_class))
    model.add(Activation("softmax"))
    model.summary()
    model.compile(loss=["categorical_crossentropy"],
    metrics=["accuracy"],optimizer=optimizer_2)
    return model
```

```{python}
model_2=build_nn()
```

```{python}
history_2=model_2.fit(X_train,y_train, 
epochs=nb_epochs,
validation_split=val_split, 
batch_size=batch_size)
```

```{python}
score_2=model_2.evaluate(X_test,y_test)
```

## A jakby sie zapominaa cz rzeczy jakich si nauczya?

Czyli co to takiego `Dropout` i dlaczego jest taki wa偶ny?

```{python}
from tensorflow.keras.layers import Dropout
```

```{python}
optimizer_3=SGD()
drp=0.3
```

```{python}

def build_nn_2():
    model=Sequential()
    
    #Warstwa wejsciowa
    model.add(Dense(nb_hidden,input_shape=(reshaped,)))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa ukryta
    model.add(Dense(nb_hidden))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa wyjsciowa
    model.add(Dense(nb_class))
    model.add(Activation("sigmoid"))
    model.summary()
    model.compile(loss=["categorical_crossentropy"],
    optimizer=optimizer_3,metrics=["accuracy"])
    return model
```

```{python}
model_3=build_nn_2()
```

```{python}
history_3=model_3.fit(X_train, y_train, epochs=nb_epochs,
validation_split=val_split, batch_size=batch_size)
```

```{python}
score_3=model_3.evaluate(X_test,y_test)
```

## Nowe optymalizatory + Dropout?

Nie tylko samym `SGD` czowiek 偶yje, czyli `RMSprop` i `Adam` w praktyce 

#### RMSprop

```{python}
from tensorflow.keras.optimizers import RMSprop

optimizer_4=RMSprop()
```

```{python}
def build_nn_with_new_optimizer():
    model=Sequential()
    
    #Warstwa wejsciowa
    model.add(Dense(nb_hidden, input_shape=(reshaped,)))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa ukryta
    model.add(Dense(nb_hidden))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa wyjsciowa
    model.add(Dense(nb_class))
    model.add(Activation("softmax"))
    model.summary()
    model.compile(optimizer=optimizer_4, 
    loss=["categorical_crossentropy"], metrics=["accuracy"])
    return model
```

```{python}
model_4=build_nn_with_new_optimizer()
```

```{python}
history_4=model_4.fit(X_train, y_train, 
epochs=nb_epochs, batch_size=batch_size,
validation_split=val_split)
```

```{python}
score_new_optimizer=model_4.evaluate(X_test,y_test)
```

#### Adam

```{python}
from tensorflow.keras.optimizers import Adam

optimizer_5=Adam()
```

```{python}
def build_nn_with_new_optimizer_v2():
    model=Sequential()
    
    #Warstwa wejsciowa
    model.add(Dense(nb_hidden, input_shape=(reshaped,)))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa ukryta
    model.add(Dense(nb_hidden))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa wyjsciowa
    model.add(Dense(nb_class))
    model.add(Activation("softmax"))
    model.summary()
    model.compile(optimizer=optimizer_5, 
    loss=["categorical_crossentropy"], metrics=["accuracy"])
    return model
```

```{python}

model_5=build_nn_with_new_optimizer_v2()
```

```{python}
history_5=model_5.fit(X_train, y_train, 
epochs=nb_epochs, batch_size=batch_size,
validation_split=val_split)
```

```{python}
score_new_optimizer=model_5.evaluate(X_test,y_test)
```

Wykres jak przebiega trening bo dawno nie byo 

```{python}
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history_5.history['accuracy'])
plt.plot(history_5.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(history_5.history['loss'])
plt.plot(history_5.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')

plt.show()
```

## Sekcja zagadek dla bystrzak贸w

```{python}
#Co jest nie tak w tej sieci?
#Dane wejsciowe s takie same, przeskalowany MNIST

model_mistery=Sequential()
    
#Warstwa wejsciowa
model_mistery.add(Dense(nb_hidden, input_shape=(reshaped,)))
model_mistery.add(Activation("relu"))
model_mistery.add(Dropout(0.99))
    
#Warstwa ukryta
model_mistery.add(Dense(nb_hidden))
model_mistery.add(Activation("relu"))
model_mistery.add(Dropout(0.99))
    
#Warstwa wyjsciowa
model_mistery.add(Dense(nb_class))
model_mistery.add(Activation("softmax"))

model_mistery.summary()
model_mistery.compile(optimizer=optimizer_5, 
loss=["categorical_crossentropy"], metrics=["accuracy"])
```

```{python}
history_mistery=history_5=model_5.fit(X_train, y_train, 
epochs=nb_epochs, batch_size=batch_size,
validation_split=val_split)
```

```{python}
score_mistery=model_mistery.evaluate(X_test,y_test)
```

![](Squish_it_mw.mp4)
