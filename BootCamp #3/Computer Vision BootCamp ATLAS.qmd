---
title: ""
author: ""
format: html
editor: visual
---

# BootCamp ATLAS #3

***Author : Mateusz Walo***

# Wstęp do uczenia głębokiego

Czym jest uczenie głębokie i jak ma się w stosunku do klasycznego uczenia maszynowego które poznałeś na poprzednich zajęciach?

![](mldl.png)

## Sieci neuronowe - idea

Czym są sieci neuronowe? Z czego się składają? Dlaczego są tak istotne w wizji komputerowej?

![](nn_image_bootcamp.png){fig-alt="Model sieci neuronowej"}

## Perceptron

![](Perceptron.png)

Prosty algorytm który dla wektora `x` o `n` wartościach, `x=(x1,x2,x3,...xn)` nazywanych w przyszłości cechami *(ang. features)* zwraca wynik 0 (fałsz) albo 1 (prawda)

$$
f(x) = 
\begin{cases} 
1 & \text{jeśli } wx + b > 0 \\
0 & \text{w każdym innym przypadku}
\end{cases}
$$

\
Gdzie `b` symbolizuje ***bias***, `w` jest wektorem wag a `wx` jest wynikiem sumy

$$
wx=\sum_{j=1}^{m} w_j x_j
$$\

***Bias** - Dodatkowy parametr dodawany do sumy ważonej w każdym neuronie, przesuwa on `funkcję aktywacji` w taki sposób aby sieć mogła dopasować się do danych*

**Spostrzeżenie:** Jak pamiętacie z Geometrii Analitycznej `wx+b` stanowi hiperpłaszczyzne graniczną która zmienia swoje położenie w przestrzeni (w tym wypadku n-wymiarowej) w zależności od `w` i `b` 😉

## Dlaczego Perceptron jest problematyczny w treningu?

Weżmy pod uwagę pojedyńczy neuron, jak wybrać najlepsze wagi i bias?

W ogólbym przypadku chcemy dostarczyć zbiór treningowy aby model sam ustalił wagę i bias, tak aby minimalizować liczbę błędów wraz z kolejnymi epokami sieci. Wyobraźmy sobie że mamy zbiór zdjęć żyraf (takie duże długoszyjne zwierze afrykańskie) i zbiór który nie zawiera ani jednej żyrafy, dokonajmy założenia że każdy neuron wejściowy przyjmuje dane z jednego piksela na zdjęciu. Podczas gdy komputer przetwarza te obrazy, chcielibyśmy, aby nasz neuron dostosowywał swoje wagi i bias, abyśmy mieli coraz mniej obrazów błędnie rozpoznawanych jako stworzenia będące nie-żyrafami. Wszystko byłoby pięknie gdyby niewielka zmiana wartości `w` i `b` powodowała tylko niewielką zmianę w predykcji sieci. Natomiast mamy duży skok wyjściowy, co uniemożliwia nam progresywne uczenie sieci. Co prezentuje wykres poniżej

![](pppp.png){fig-align="center" width="500"}

Matematycy obecni na sali z łatwością zauważą że dzieję się tak ponieważ ta funkcja nie jest ciągła w zerze 😉

## Ratunek - funkcje aktywacyjne

### 

***Sigmoid*** - tak nazywamy funkcję aktywacyjną daną wzorem

$$
\sigma(x)=\dfrac{1}{1+e^{-x}}
$$

Jej wykres prezentuje się następująco

![Jak widzimy w tym wypadku funkcja jest ciągła w zerze, co pozwala sieci na odpowiedź "może" gdy nie jest pewna swojej predykcji w trudniejszych przypadkach. Jak dobrze pamiętacie z algebry liniowej istnieje coś takiego jak kombinacja liniowa, bez funkcji aktywacji wagi i bias przekazywane do nastepnych warstw bylyby tyko kombinacją liniową, nie zmieniałyby swoich wartości i siec nie mogłaby się uczyć.](Logistic-curve.png){fig-align="center"}

**ReLU -**tak nazywamy funkcję daną wzorem

$$
f(x)=max(0,x)
$$

której wykres prezentuje się następująco

![](relu.png){fig-align="center"}

***Dla dociekliwych:** Istnieje kilka innych funkcji aktywacyjnych, które omówimy szczegółowo w dalszej części BootCampu 😉*

## Funkcje straty

**MSE** - To jest średni błąd kwadratowy między przewidywaniami a prawdziwymi wartościami. Matematycznie, jeśli `Y'` jest wektorem `n` predykcji, a `Y` jest wektorem `n` obserwowanych wartości, to spełniają one następujące równanie:

$$
MSE=\dfrac{1}{n}\sum_{i=1}^{n}(Y'-Y)^2
$$

**Spostrzeżenie:** *Dzięki tej funkcji możemy dostrzec że jeśli predykcje będą się różnić znacząco od wartości prawdziwych, to roznica bedzie widocza drastycznie, zobaczymy ten błąd do kwadratu!*

**Binary cross-entropy** - Jest to binarna logarytmiczna strata. Załóżmy, że predykcja modelu to `p`, podczas gdy naszym targetem jest `t` wówczas binarna entropia krzyżowa jest zdefiniowana następująco:

$$
-tln(p)-(1-t)ln(1-p)
$$

**Spostrzeżenie:** Wykorzystywana najczęściej do klasyfikacji binarnej (na studiach będziecie jej używać najczęściej 😉)

**Categorical cross-entropy -** oznaczenia podobnie jak wyżej

$$
L_i=-\sum jt_{i,j}ln(p_{i,j})
$$

**Spostrzeżenie:** Wykorzystywana najczęściej do klasyfikacji wieloetykietowej (zaraz zobaczycie ją w akcji)

## Optymalizator

Wyobraźmy sobie ogólną funkcję kosztu C(w) jednej zmiennej jak na poniższym wykresie:

![](opt.png){fig-align="center"}

Schodzenie po gradiencie można postrzegać jako wędrowca, który zamierza zejść z góry do doliny. Góra reprezentuje funkcję `C`, podczas gdy dolina reprezentuje minimalne `min` . Wędrowiec ma punkt początkowy `w0` . Wędrowiec porusza się powoli. Na każdym kroku `r` gradient jest kierunkiem maksymalnego wzrostu. Matematycznie ten kierunek jest wartością pochodnej cząstkowej w punkcie `wr` osiągniętym w kroku `r`. Dlatego też, wybierając przeciwny kierunek, wędrowiec może poruszać się w kierunku doliny. Na każdym kroku wędrowiec może decydować, jaką odległośc chce pokonać następnym krokiem. Jest to tempo uczenia się. Należy zauważyć, że jeśli jest zbyt małe, wędrowiec będzie poruszał się powoli. Jednak jeśli jest zbyt duże, wędrowiec prawdopodobnie ominie dolinę.

## Realny przykład - Baza MNIST

Baza danych MNIST zawiera 70 000 zdjęc odręcznie pisanych cyfr, w skali szarości, każde wymiaru 28x28 pikseli.

Jak zaimportować tensorflow i MNIST?

```{python}
!pip install -U numpy --quiet
```

```{python}
#!pip install -U tensorflow pandas matplotlib --quiet
#Instrukcja powyżej jeśli nie masz zainstalowanych bibliotek

#Importujemy potrzebne biblioteki

from tensorflow.keras.models import Sequential
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.utils import to_categorical
import numpy as np 
import matplotlib.pyplot as plt
```

```{python}
#Ustalamy wartość podstawowych parametrów

nb_epochs=10 #Liczba epok przez które będzie się trenować sieć
val_split=0.2 #Czesc jaka zostanie przydzielona na zbior walidacyjny
nb_class=10 #Liczba klas outputów, cyfry of 0 do 9
nb_hidden=128 #Liczba warstw ukrytych
reshaped=784 #do skalowania zdjęć, 784=28*28 co odpowiada wymiarowi zdjęc
batch_size=128 #Liczba próbek która będzie przetwarzana w 1 obrocie
optimizer=SGD() #Optymalizator Stochastic gradient descent
```

```{python}
#Wczytujemy dane mnist

(X_train, y_train),(X_test, y_test)=mnist.load_data()
```

```{python}
#Sprawdzmy czy dane wczytały się poprawnie

plt.figure()
plt.imshow(X_train[1])
plt.title(f"Na rysunku znajduję się cyfra {y_train[1]}")
plt.tight_layout()
plt.show()
```

```{python}
# Przekształcamy, normalizujemy dane mnist

X_train=X_train.reshape(60000, reshaped)
X_test=X_test.reshape(10000, reshaped)
X_train=X_train.astype("float32")/255
X_test=X_test.astype("float32")/255
y_train=to_categorical(y_train, nb_class) #Pamiętacie co to OneHotEncoding ;)?
y_test=to_categorical(y_test, nb_class)
```

```{python}
#Budujemy prostą sieć neuronową posługując się podejściem funkcyjnym

def build_simple_nn():
    model=Sequential()
    
    #Warstwa wejsciowo/wyjsciowa
    model.add(Dense(nb_class,input_shape=(reshaped,)))
    
    model.add(Activation("sigmoid"))
    model.summary()
    model.compile(loss=["categorical_crossentropy"], 
    metrics=["accuracy"],optimizer=optimizer)
    return model
```

-   Istnieje możliwośc dodania funkcji aktywacji bezpośrednio w Dense layer za pomocą parametru `activation` lecz w ramach nauki dodajemy ją osobno aby pamiętać o strukturze budowania sieci

```{python}
model=build_simple_nn()

```

```{python}
history=model.fit(X_train, y_train,
batch_size=batch_size, epochs=nb_epochs,
validation_split=val_split)
```

```{python}

score=model.evaluate(X_test, y_test)
```

```{python}
#Pokazujemy jak przebiegał trening naszej prostej sieci
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')

plt.show()
```

**Brawo, właśnie wytrenowaliście i dokonaliście ewaluacji swojej pierwszej sieci neuronowej 🥳🥳🥳**

## A możeby tak zrobić lepszą sieć?

Czyli warstwy ukryte w akcji!

```{python}
#ustawmy nowy optymalizator dla nowej sieci, pozostala czesc parametrów
#zostanie bez zmian

optimizer_2=SGD()
```

```{python}
def build_nn():
    model=Sequential()
    
    #Warstwa wejsciowa
    model.add(Dense(nb_hidden, input_shape=(reshaped,)))
    model.add(Activation("relu"))
    
    #Warstwa ukryta
    model.add(Dense(nb_hidden))
    model.add(Activation("relu"))
    
    #Warstwa output
    model.add(Dense(nb_class))
    model.add(Activation("softmax"))
    model.summary()
    model.compile(loss=["categorical_crossentropy"],
    metrics=["accuracy"],optimizer=optimizer_2)
    return model
```

```{python}
model_2=build_nn()
```

```{python}
history_2=model_2.fit(X_train,y_train, 
epochs=nb_epochs,
validation_split=val_split, 
batch_size=batch_size)
```

```{python}
score_2=model_2.evaluate(X_test,y_test)
```

## A jakby sieć zapominała część rzeczy jakich się nauczyła?

Czyli co to takiego `Dropout` i dlaczego jest taki ważny?

```{python}
from tensorflow.keras.layers import Dropout
```

```{python}
optimizer_3=SGD()
drp=0.3
```

```{python}

def build_nn_2():
    model=Sequential()
    
    #Warstwa wejsciowa
    model.add(Dense(nb_hidden,input_shape=(reshaped,)))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa ukryta
    model.add(Dense(nb_hidden))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa wyjsciowa
    model.add(Dense(nb_class))
    model.add(Activation("sigmoid"))
    model.summary()
    model.compile(loss=["categorical_crossentropy"],
    optimizer=optimizer_3,metrics=["accuracy"])
    return model
```

```{python}
model_3=build_nn_2()
```

```{python}
history_3=model_3.fit(X_train, y_train, epochs=nb_epochs,
validation_split=val_split, batch_size=batch_size)
```

```{python}
score_3=model_3.evaluate(X_test,y_test)
```

## Nowe optymalizatory + Dropout?

Nie tylko samym `SGD` człowiek żyje, czyli `RMSprop` i `Adam` w praktyce 😄

#### RMSprop

```{python}
from tensorflow.keras.optimizers import RMSprop

optimizer_4=RMSprop()
```

```{python}
def build_nn_with_new_optimizer():
    model=Sequential()
    
    #Warstwa wejsciowa
    model.add(Dense(nb_hidden, input_shape=(reshaped,)))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa ukryta
    model.add(Dense(nb_hidden))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa wyjsciowa
    model.add(Dense(nb_class))
    model.add(Activation("softmax"))
    model.summary()
    model.compile(optimizer=optimizer_4, 
    loss=["categorical_crossentropy"], metrics=["accuracy"])
    return model
```

```{python}
model_4=build_nn_with_new_optimizer()
```

```{python}
history_4=model_4.fit(X_train, y_train, 
epochs=nb_epochs, batch_size=batch_size,
validation_split=val_split)
```

```{python}
score_new_optimizer=model_4.evaluate(X_test,y_test)
```

#### Adam

```{python}
from tensorflow.keras.optimizers import Adam

optimizer_5=Adam()
```

```{python}
def build_nn_with_new_optimizer_v2():
    model=Sequential()
    
    #Warstwa wejsciowa
    model.add(Dense(nb_hidden, input_shape=(reshaped,)))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa ukryta
    model.add(Dense(nb_hidden))
    model.add(Activation("relu"))
    model.add(Dropout(drp))
    
    #Warstwa wyjsciowa
    model.add(Dense(nb_class))
    model.add(Activation("softmax"))
    model.summary()
    model.compile(optimizer=optimizer_5, 
    loss=["categorical_crossentropy"], metrics=["accuracy"])
    return model
```

```{python}

model_5=build_nn_with_new_optimizer_v2()
```

```{python}
history_5=model_5.fit(X_train, y_train, 
epochs=nb_epochs, batch_size=batch_size,
validation_split=val_split)
```

```{python}
score_new_optimizer=model_5.evaluate(X_test,y_test)
```

Wykres jak przebiegał trening bo dawno nie było 😉

```{python}
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history_5.history['accuracy'])
plt.plot(history_5.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(history_5.history['loss'])
plt.plot(history_5.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')

plt.show()
```

## Sekcja zagadek dla bystrzaków

```{python}
#Co jest nie tak w tej sieci?
#Dane wejsciowe są takie same, przeskalowany MNIST

model_mistery=Sequential()
    
#Warstwa wejsciowa
model_mistery.add(Dense(nb_hidden, input_shape=(reshaped,)))
model_mistery.add(Activation("relu"))
model_mistery.add(Dropout(0.99))
    
#Warstwa ukryta
model_mistery.add(Dense(nb_hidden))
model_mistery.add(Activation("relu"))
model_mistery.add(Dropout(0.99))
    
#Warstwa wyjsciowa
model_mistery.add(Dense(nb_class))
model_mistery.add(Activation("softmax"))

model_mistery.summary()
model_mistery.compile(optimizer=optimizer_5, 
loss=["categorical_crossentropy"], metrics=["accuracy"])
```

```{python}
history_mistery=history_5=model_5.fit(X_train, y_train, 
epochs=nb_epochs, batch_size=batch_size,
validation_split=val_split)
```

```{python}
score_mistery=model_mistery.evaluate(X_test,y_test)
```

![](Squish_it_mw.mp4)
