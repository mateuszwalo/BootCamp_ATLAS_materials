plt.scatter(y_test, y_pred_lr, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.show()
# Potrzebne biblioteki
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from imblearn.metrics import specificity_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
# Wgrywamy dane
data = pd.read_csv('possum.csv')
data.head()
# Usuwamy braki danych
data = data.dropna()
# Usuwamy zmienną 'case' - identyfikator oposa
data = data.drop('case', axis=1)
# Usuwamy zmienne, które nie są wymiarami oposa
data = data.drop(['site','Pop','sex','age'], axis=1)
# X - zmienne niezależne (predyktory), y - zmienna zależna (wynikowa)
X = data.drop('hdlngth', axis=1)
y = data['hdlngth']
# Dzielimy zbiór na uczący i testowy
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Tworzymy model regresji liniowej
lr_model = LinearRegression()
# Trenujemy model na danych treningowych
lr_model.fit(X_train, y_train)
# Przewidujemy wartość zmiennej zależnej dla danych testowych
y_pred_lr = lr_model.predict(X_test)
r2_score(y_test, y_pred_lr)
np.sqrt(mean_squared_error(y_test, y_pred_lr))
mean_absolute_error(y_test, y_pred_lr)
# Tworzymy model XGBoost
xgbr_model = XGBRegressor()
xgbr_model.fit(X_train, y_train)
y_pred_xgbr = xgbr_model.predict(X_test)
r2_score(y_test, y_pred_xgbr)
np.sqrt(mean_squared_error(y_test, y_pred_xgbr))
mean_absolute_error(y_test, y_pred_xgbr)
plt.scatter(y_test, y_pred_lr, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.show()
plt.scatter(y_test, y_pred_lr, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.show()
plt.scatter(y_test, y_pred_lr, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.title('Wykres wartości dopasowanych i obserwowanych')
plt.show()
plt.scatter(y_test, y_pred_lr, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.title('Wykres wartości dopasowanych i obserwowanych')
plt.show()
plt.scatter(y_test, y_pred_lr, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.title('Wykres wartości dopasowanych i obserwowanych')
plt.grid()
plt.show()
plt.scatter(y_test, y_pred_lr, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.title('Wykres wartości dopasowanych i obserwowanych')
plt.grid()
plt.show()
plt.scatter(y_test, y_pred_lr, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.title('Wykres wartości dopasowanych i obserwowanych')
plt.grid()
plt.show()
plt.scatter(y_test, y_pred_lr, color='blue', label='Ok')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.title('Wykres wartości dopasowanych i obserwowanych')
plt.grid()
plt.show()
plt.scatter(y_test, y_pred_lr, color='blue', label='Ok')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.title('Wykres wartości dopasowanych i obserwowanych')
plt.grid()
plt.show()
plt.scatter(y_test, y_pred_lr, color='blue', label='Ok')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.title('Wykres wartości dopasowanych i obserwowanych')
plt.legend()
plt.grid()
plt.show()
plt.scatter(y_test, y_pred_lr, color='blue', label='Ok')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.title('Wykres wartości dopasowanych i obserwowanych')
plt.legend()
plt.grid()
plt.show()
plt.scatter(y_test, y_pred_lr, color='blue', label='Ok')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.title('Wykres wartości dopasowanych i obserwowanych')
plt.legend()
plt.grid()
plt.show()
plt.scatter(y_test, y_pred_lr, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.title('Wykres wartości dopasowanych i obserwowanych')
plt.grid()
plt.show()
plt.scatter(y_test, y_pred_lr, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.title('Wykres wartości dopasowanych i obserwowanych')
plt.grid()
plt.show()
# Wgrywamy dane
data = pd.read_csv('possum.csv')
data.head()
# Usuwamy braki danych
data = data.dropna()
# Usuwamy zmienną 'case' - identyfikator oposa
data = data.drop('case', axis=1)
# Usuwamy zmienne, które nie są wymiarami oposa
data = data.drop(['site','Pop','sex','age'], axis=1)
# X - zmienne niezależne (predyktory), y - zmienna zależna (wynikowa)
X = data.drop('hdlngth', axis=1)
y = data['hdlngth']
# Dzielimy zbiór na uczący i testowy
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Tworzymy model regresji liniowej
lr_model = LinearRegression()
# Trenujemy model na danych treningowych
lr_model.fit(X_train, y_train)
# Przewidujemy wartość zmiennej zależnej dla danych testowych
y_pred_lr = lr_model.predict(X_test)
r2_score(y_test, y_pred_lr)
np.sqrt(mean_squared_error(y_test, y_pred_lr))
mean_absolute_error(y_test, y_pred_lr)
# Tworzymy model XGBoost
xgbr_model = XGBRegressor()
xgbr_model.fit(X_train, y_train)
y_pred_xgbr = xgbr_model.predict(X_test)
r2_score(y_test, y_pred_xgbr)
np.sqrt(mean_squared_error(y_test, y_pred_xgbr))
mean_absolute_error(y_test, y_pred_xgbr)
plt.scatter(y_test, y_pred_lr, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Idealne dopasowanie')
plt.xlabel('Wartości obserwowane')
plt.ylabel('Wartości dopasowane')
plt.title('Wykres wartości dopasowanych i obserwowanych')
plt.grid()
plt.show()
reticulate::repl_python()
